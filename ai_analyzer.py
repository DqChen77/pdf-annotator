"""
AIåˆ†ææ¨¡å— - ä½¿ç”¨OpenAI APIåˆ†ææ–‡æœ¬å†…å®¹
"""
import json
import time
from typing import List, Dict, Optional
from openai import OpenAI
import tiktoken
from datetime import datetime
import os
from config import (
    get_openai_api_key,
    get_openai_model,
    get_openai_base_url,
    SYSTEM_PROMPT,
    ANALYSIS_PROMPT,
    MAX_TOKENS_PER_CHUNK
)


class AIAnalyzer:
    """AIæ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        åˆå§‹åŒ–AIåˆ†æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        # åŠ¨æ€è·å–é…ç½®ï¼ˆæ”¯æŒè¿è¡Œæ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
        self.api_key = api_key or get_openai_api_key()
        self.model = model or get_openai_model()
        
        if not self.api_key:
            raise ValueError("è¯·é…ç½®OPENAI_API_KEYï¼ˆåœ¨GUIä¸­è¾“å…¥æˆ–åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®ï¼‰")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client_args = {"api_key": self.api_key}
        base_url = get_openai_base_url()
        if base_url:
            client_args["base_url"] = base_url
        
        self.client = OpenAI(**client_args)
        
        # åˆå§‹åŒ–tokenizer
        try:
            self.encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°"""
        return len(self.encoding.encode(text))
    
    def chunk_text(self, text: str, max_tokens: int = MAX_TOKENS_PER_CHUNK) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_tokens: æ¯å—æœ€å¤§tokenæ•°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            test_chunk = current_chunk + "\n\n" + para if current_chunk else para
            
            if self.count_tokens(test_chunk) <= max_tokens:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _save_ai_response_log(self, result_text: str, result: Dict):
        """
        ä¿å­˜AIå“åº”åˆ°æ—¥å¿—æ–‡ä»¶
        
        Args:
            result_text: AIè¿”å›çš„åŸå§‹JSONæ–‡æœ¬
            result: è§£æåçš„å­—å…¸
        """
        try:
            # åˆ›å»ºlogsç›®å½•
            log_dir = "logs"
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = os.path.join(log_dir, f"ai_response_{timestamp}.json")
            
            # æ£€æµ‹é‡å¤æœ¯è¯­
            term_texts = [t.get('text', '').lower().strip() for t in result.get('terms', [])]
            duplicates = {}
            for t in term_texts:
                if term_texts.count(t) > 1:
                    duplicates[t] = term_texts.count(t)
            
            # å‡†å¤‡æ—¥å¿—å†…å®¹
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "model": self.model,
                "raw_response": result_text,
                "parsed_result": result,
                "statistics": {
                    "highlights_count": len(result.get("highlights", [])),
                    "terms_count": len(result.get("terms", [])),
                    "unique_terms_count": len(set(term_texts)),
                    "duplicates": duplicates,
                    "summaries_count": len(result.get("summaries", []))
                },
                "all_term_texts": [t.get('text', '') for t in result.get('terms', [])]
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ“ AIå“åº”å·²ä¿å­˜åˆ°: {log_file}")
            
        except Exception as e:
            print(f"   âš ï¸  ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")
    
    def analyze_text(self, text: str, retry_count: int = 3, custom_prompt: Optional[str] = None) -> Dict:
        """
        åˆ†ææ–‡æœ¬ï¼Œè¯†åˆ«å…³é”®ç‚¹å’Œç”Ÿæˆæ€»ç»“
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            retry_count: é‡è¯•æ¬¡æ•°
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«highlightså’Œsummary
        """
        prompt_template = custom_prompt if custom_prompt else ANALYSIS_PROMPT
        prompt = prompt_template.format(text=text)
        
        for attempt in range(retry_count):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                
                result_text = response.choices[0].message.content
                
                # æ‰“å°åŸå§‹å“åº”ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                print(f"\n   ğŸ“‹ AIåŸå§‹å“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰:")
                print(f"   {result_text[:500]}...")
                
                result = json.loads(result_text)
                
                # ä¿å­˜å®Œæ•´å“åº”åˆ°æ—¥å¿—æ–‡ä»¶
                self._save_ai_response_log(result_text, result)
                
                # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                print(f"\n   ğŸ“Š AIè¿”å›å†…å®¹ç»Ÿè®¡:")
                print(f"   - highlights: {len(result.get('highlights', []))} ä¸ª")
                print(f"   - terms: {len(result.get('terms', []))} ä¸ª")
                print(f"   - summaries: {len(result.get('summaries', []))} ä¸ª")
                
                # æ£€æµ‹é‡å¤æœ¯è¯­
                if result.get('terms'):
                    print(f"\n   ğŸ“ å‰10ä¸ªæœ¯è¯­ç¤ºä¾‹:")
                    for i, term in enumerate(result['terms'][:10], 1):
                        print(f"      {i}. {term.get('text', 'N/A')}")
                    
                    # æ£€æŸ¥é‡å¤
                    term_texts = [t.get('text', '').lower().strip() for t in result['terms']]
                    duplicates = [t for t in set(term_texts) if term_texts.count(t) > 1]
                    if duplicates:
                        print(f"\n   âš ï¸âš ï¸âš ï¸  æ£€æµ‹åˆ°é‡å¤æœ¯è¯­ï¼AIåœ¨å……æ•°ï¼")
                        print(f"   é‡å¤çš„æœ¯è¯­: {', '.join(duplicates[:5])}")
                        if len(duplicates) > 5:
                            print(f"   è¿˜æœ‰ {len(duplicates)-5} ä¸ªé‡å¤æœ¯è¯­...")
                    
                    # å10ä¸ªæœ¯è¯­ï¼ˆæ£€æŸ¥æ˜¯å¦åœ¨å‡‘æ•°ï¼‰
                    if len(result['terms']) > 10:
                        print(f"\n   ğŸ“ å10ä¸ªæœ¯è¯­ç¤ºä¾‹ï¼ˆæ£€æŸ¥è´¨é‡ï¼‰:")
                        for i, term in enumerate(result['terms'][-10:], len(result['terms'])-9):
                            print(f"      {i}. {term.get('text', 'N/A')}")
                
                # éªŒè¯ç»“æœæ ¼å¼
                if "highlights" not in result:
                    result["highlights"] = []
                
                # ç¡®ä¿æ¯ä¸ªhighlightéƒ½æœ‰noteå­—æ®µ
                for highlight in result["highlights"]:
                    if "note" not in highlight and "reason" in highlight:
                        highlight["note"] = highlight["reason"]
                    elif "note" not in highlight:
                        highlight["note"] = ""
                
                return result
                
            except json.JSONDecodeError as e:
                print(f"JSONè§£æé”™è¯¯ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt == retry_count - 1:
                    return {"highlights": [], "summary": ""}
                time.sleep(1)
                
            except Exception as e:
                print(f"APIè°ƒç”¨é”™è¯¯ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt == retry_count - 1:
                    return {"highlights": [], "summary": ""}
                time.sleep(2)
        
        return {"highlights": [], "summary": ""}
    
    def analyze_document_full(self, text_blocks: List[Dict], progress_callback=None, custom_prompt: Optional[str] = None) -> List[Dict]:
        """
        é€šè¯»å…¨æ–‡ååˆ†ææ•´ä¸ªæ–‡æ¡£ï¼ˆå…¨å±€è§†è§’ï¼‰
        
        Args:
            text_blocks: æ–‡æœ¬å—åˆ—è¡¨ï¼ˆæ¥è‡ªPDFï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«éœ€è¦é«˜äº®çš„å†…å®¹å’Œå¯¹åº”çš„æ–‡æœ¬å—
        """
        # ç¬¬ä¸€æ­¥ï¼šç»„åˆå…¨æ–‡
        print("   - æ­£åœ¨æå–å…¨æ–‡...")
        full_text = "\n\n".join([block["text"] for block in text_blocks if len(block["text"].strip()) > 50])
        
        # è®¡ç®—tokenæ•°
        token_count = self.count_tokens(full_text)
        print(f"   - å…¨æ–‡tokenæ•°: {token_count}")
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†é•¿æ–‡æœ¬
        if token_count > MAX_TOKENS_PER_CHUNK:
            print(f"   - æ–‡æœ¬è¾ƒé•¿ï¼Œåˆ†æ®µåˆ†æï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰...")
            return self._analyze_long_document(full_text, text_blocks, progress_callback)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¸€æ¬¡æ€§åˆ†æå…¨æ–‡
        print("   - æ­£åœ¨é€šè¯»å…¨æ–‡å¹¶è¯†åˆ«å…³é”®è§‚ç‚¹...")
        analysis = self.analyze_text(full_text, custom_prompt=custom_prompt)
        
        # ç«‹å³å¯¹æœ¯è¯­è¿›è¡Œå»é‡ï¼ˆåœ¨æ˜ å°„å‰ï¼‰
        original_term_count = len(analysis.get("terms", []))
        if "terms" in analysis:
            analysis["terms"] = self._deduplicate_terms(analysis["terms"])
        
        # æ£€æŸ¥è¿”å›çš„é«˜äº®æ•°é‡
        highlight_count = len(analysis.get("highlights", []))
        term_count = len(analysis.get("terms", []))
        summary_count = len(analysis.get("summaries", []))
        print(f"   - AIè¿”å›äº† {highlight_count} ä¸ªé«˜äº®è§‚ç‚¹")
        print(f"   - AIè¿”å›äº† {original_term_count} ä¸ªä¸“ä¸šæœ¯è¯­ï¼ˆå«é‡å¤ï¼‰")
        if original_term_count != term_count:
            print(f"   - å»é‡åä¿ç•™ {term_count} ä¸ªå”¯ä¸€æœ¯è¯­")
        else:
            print(f"   - æ— é‡å¤æœ¯è¯­")
        print(f"   - AIè¿”å›äº† {summary_count} ä¸ªæ®µè½æ€»ç»“")
        
        if highlight_count < 15:
            print(f"   âš ï¸  è­¦å‘Šï¼šé«˜äº®æ•°é‡å°‘äºé¢„æœŸï¼ˆåº”è¯¥æœ‰20-30ä¸ªï¼‰")
        if term_count < 30:
            print(f"   âš ï¸  æç¤ºï¼šæœ¯è¯­æ•°é‡è¾ƒå°‘ï¼ˆå»é‡å{term_count}ä¸ªï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´å…¨é¢çš„æ ‡æ³¨")
        if summary_count < 3:
            print(f"   âš ï¸  è­¦å‘Šï¼šæ®µè½æ€»ç»“å°‘äºé¢„æœŸï¼ˆåº”è¯¥æœ‰5-10ä¸ªï¼‰")
        
        if progress_callback:
            progress_callback(1, 1)
        
        # ç¬¬å››æ­¥ï¼šå°†é«˜äº®å†…å®¹æ˜ å°„å›å¯¹åº”çš„æ–‡æœ¬å—
        print("   - æ­£åœ¨å®šä½é«˜äº®ä½ç½®...")
        results = self._map_highlights_to_blocks(analysis, text_blocks)
        
        # ç¬¬äº”æ­¥ï¼šå°†æœ¯è¯­æ˜ å°„å›å¯¹åº”çš„æ–‡æœ¬å—
        print("   - æ­£åœ¨å®šä½æœ¯è¯­ä½ç½®...")
        term_results = self._map_terms_to_blocks(analysis, text_blocks)
        
        # åˆå¹¶ç»“æœ
        all_results = results + term_results
        
        print(f"   - æˆåŠŸæ˜ å°„ {len(results)} ä¸ªè§‚ç‚¹é«˜äº®åˆ°PDFä¸­")
        print(f"   - æˆåŠŸæ˜ å°„ {len(term_results)} ä¸ªæœ¯è¯­é«˜äº®åˆ°PDFä¸­")
        
        # ä¿å­˜æ®µè½æ€»ç»“ä»¥ä¾¿åç»­ä½¿ç”¨
        self._cached_summaries = analysis.get("summaries", [])
        
        return all_results
    
    def get_cached_summaries(self):
        """è·å–ç¼“å­˜çš„æ®µè½æ€»ç»“"""
        return getattr(self, '_cached_summaries', [])
    
    def _analyze_long_document(self, full_text: str, text_blocks: List[Dict], progress_callback=None) -> List[Dict]:
        """
        åˆ†æé•¿æ–‡æ¡£ï¼ˆåˆ†æ®µä½†ä¿æŒå…¨å±€è§†è§’ï¼‰
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            text_blocks: æ–‡æœ¬å—åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            åˆ†æç»“æœ
        """
        # å°†æ–‡æœ¬åˆ†æˆå¤§å—ï¼ˆæ¯å—çº¦6000 tokensï¼Œæ¯”å•æ®µå¤§å¾—å¤šï¼‰
        chunk_size = 6000
        chunks = self.chunk_text(full_text, max_tokens=chunk_size)
        
        print(f"   - åˆ†æˆ{len(chunks)}ä¸ªå¤§æ®µè¿›è¡Œåˆ†æ...")
        
        all_highlights = []
        
        # åˆ†ææ¯ä¸ªå¤§æ®µ
        for i, chunk in enumerate(chunks):
            print(f"   - åˆ†æç¬¬{i+1}/{len(chunks)}æ®µ...")
            
            analysis = self.analyze_text(chunk)
            
            if analysis.get("highlights"):
                chunk_highlights = len(analysis["highlights"])
                print(f"     è¿”å› {chunk_highlights} ä¸ªè§‚ç‚¹")
                all_highlights.extend(analysis["highlights"])
            
            if progress_callback:
                progress_callback(i + 1, len(chunks))
            
            # é¿å…APIé™æµ
            time.sleep(0.5)
        
        print(f"   - æ€»å…±è·å¾— {len(all_highlights)} ä¸ªè§‚ç‚¹")
        
        # åˆå¹¶ç»“æœå¹¶æ˜ å°„åˆ°æ–‡æœ¬å—
        combined_analysis = {"highlights": all_highlights}
        results = self._map_highlights_to_blocks(combined_analysis, text_blocks)
        
        return results
    
    def _deduplicate_terms(self, terms: List[Dict]) -> List[Dict]:
        """
        æ™ºèƒ½å»é‡æœ¯è¯­åˆ—è¡¨ï¼ˆå¤„ç†å®Œå…¨é‡å¤å’ŒåŒ…å«å…³ç³»ï¼‰- å¢å¼ºç‰ˆ
        
        Args:
            terms: æœ¯è¯­åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æœ¯è¯­åˆ—è¡¨
        """
        if not terms:
            return []
        
        unique_terms = []
        seen_keys = set()
        seen_texts = []
        
        for term in terms:
            term_text = term.get("text", "").strip()
            if not term_text:
                continue
            
            # æ ‡å‡†åŒ–ï¼šå°å†™ + ç§»é™¤å¤šä½™ç©ºæ ¼
            term_key = ' '.join(term_text.lower().split())
            
            # æ£€æŸ¥æ˜¯å¦å®Œå…¨é‡å¤
            if term_key in seen_keys:
                continue
            
            # æ£€æŸ¥åŒ…å«å…³ç³»ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            should_skip = False
            term_to_replace = None
            
            # åˆ†è¯æ¯”è¾ƒï¼ˆå¤„ç† "nucleus accumbens" è¿™ç±»å¤šè¯æœ¯è¯­ï¼‰
            term_words = set(term_key.split())
            
            for i, seen_text in enumerate(seen_texts):
                seen_key = ' '.join(seen_text.lower().split())
                seen_words = set(seen_key.split())
                
                # å®Œå…¨ç›¸åŒï¼ˆç»è¿‡æ ‡å‡†åŒ–åï¼‰
                if term_key == seen_key:
                    should_skip = True
                    break
                
                # å¤šè¯æœ¯è¯­çš„å®Œå…¨åŒ…å«ï¼ˆå¦‚ "nucleus accumbens" vs "nucleus")
                # å¦‚æœæ–°æœ¯è¯­çš„æ‰€æœ‰è¯éƒ½åœ¨æ—§æœ¯è¯­ä¸­ï¼Œä¸”è¯åºç›¸åŒ
                if term_key in seen_key and len(term_words) < len(seen_words):
                    # æ–°æœ¯è¯­æ˜¯å­é›†ï¼Œä¿ç•™æ—§çš„
                    should_skip = True
                    break
                elif seen_key in term_key and len(seen_words) < len(term_words):
                    # æ—§æœ¯è¯­æ˜¯å­é›†ï¼Œä¿ç•™æ–°çš„
                    term_to_replace = i
                    break
                
                # å¤„ç†ç›¸åŒæ ¸å¿ƒè¯çš„ä¸åŒå½¢å¼
                # å¦‚ "nucleus accumbens" çš„å„ç§å‡ºç°
                if len(term_words) >= 2 and len(seen_words) >= 2:
                    # æ£€æŸ¥ä¸»è¦è¯æ˜¯å¦ç›¸åŒï¼ˆå–æœ€é•¿çš„ä¸¤ä¸ªè¯ï¼‰
                    term_main = sorted(term_words, key=len, reverse=True)[:2]
                    seen_main = sorted(seen_words, key=len, reverse=True)[:2]
                    if set(term_main) == set(seen_main):
                        # è®¤ä¸ºæ˜¯åŒä¸€æœ¯è¯­ï¼Œä¿ç•™æ›´é•¿/æ›´å®Œæ•´çš„ç‰ˆæœ¬
                        if len(term_key) > len(seen_key):
                            term_to_replace = i
                        else:
                            should_skip = True
                        break
            
            if should_skip:
                continue
            
            # å¦‚æœéœ€è¦æ›¿æ¢æ—§æœ¯è¯­
            if term_to_replace is not None:
                old_text = seen_texts[term_to_replace]
                old_key = ' '.join(old_text.lower().split())
                # ç§»é™¤æ—§çš„
                seen_keys.discard(old_key)
                seen_texts.pop(term_to_replace)
                unique_terms.pop(term_to_replace)
            
            # æ·»åŠ æ–°æœ¯è¯­
            seen_keys.add(term_key)
            seen_texts.append(term_text)
            unique_terms.append(term)
        
        return unique_terms
    
    def _smart_text_match(self, search_text: str, target_text: str) -> bool:
        """
        æ™ºèƒ½æ–‡æœ¬åŒ¹é…ï¼ˆæ›´å®½å®¹çš„åŒ¹é…ç­–ç•¥ï¼‰
        
        Args:
            search_text: è¦æœç´¢çš„æ–‡æœ¬
            target_text: ç›®æ ‡æ–‡æœ¬
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        import string
        
        # æ¸…ç†å‡½æ•°
        def normalize(text):
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            text = ' '.join(text.split())
            # è½¬å°å†™
            text = text.lower()
            return text
        
        # ç­–ç•¥1: ç›´æ¥åŒ¹é…
        if search_text in target_text:
            return True
        
        # ç­–ç•¥2: æ¸…ç†ååŒ¹é…
        norm_search = normalize(search_text)
        norm_target = normalize(target_text)
        if norm_search in norm_target:
            return True
        
        # ç­–ç•¥3: ç§»é™¤æ ‡ç‚¹ååŒ¹é…
        no_punct_search = search_text.translate(str.maketrans('', '', string.punctuation))
        no_punct_target = target_text.translate(str.maketrans('', '', string.punctuation))
        if normalize(no_punct_search) in normalize(no_punct_target):
            return True
        
        # ç­–ç•¥4: å‰70%åŒ¹é…
        if len(search_text) > 40:
            partial = search_text[:int(len(search_text) * 0.7)]
            if normalize(partial) in norm_target:
                return True
        
        # ç­–ç•¥5: å‰50%åŒ¹é…
        if len(search_text) > 30:
            partial = search_text[:int(len(search_text) * 0.5)]
            if normalize(partial) in norm_target:
                return True
        
        # ç­–ç•¥6: å•è¯çº§åˆ«åŒ¹é…ï¼ˆå‰10ä¸ªå•è¯ï¼‰
        words = search_text.split()
        if len(words) > 10:
            partial = ' '.join(words[:10])
            if normalize(partial) in norm_target:
                return True
        
        return False
    
    def _map_highlights_to_blocks(self, analysis: Dict, text_blocks: List[Dict]) -> List[Dict]:
        """
        å°†é«˜äº®å†…å®¹æ˜ å°„åˆ°å¯¹åº”çš„æ–‡æœ¬å—å’Œé¡µç ï¼ˆä½¿ç”¨æ™ºèƒ½åŒ¹é…ï¼‰
        
        Args:
            analysis: AIåˆ†æç»“æœ
            text_blocks: æ–‡æœ¬å—åˆ—è¡¨
            
        Returns:
            åŒ…å«ä½ç½®ä¿¡æ¯çš„ç»“æœåˆ—è¡¨
        """
        results = []
        unmatched = []
        
        for highlight_item in analysis.get("highlights", []):
            highlight_text = highlight_item.get("text", "")
            
            if not highlight_text:
                continue
            
            # åœ¨æ–‡æœ¬å—ä¸­æŸ¥æ‰¾è¿™æ®µæ–‡å­—
            found = False
            best_match = None
            best_match_score = 0
            
            for block in text_blocks:
                block_text = block["text"]
                
                # ä½¿ç”¨æ™ºèƒ½åŒ¹é…
                if self._smart_text_match(highlight_text, block_text):
                    # è®¡ç®—åŒ¹é…åº¦ï¼ˆç®€å•çš„åŸºäºé•¿åº¦ï¼‰
                    match_score = len(highlight_text)
                    if match_score > best_match_score:
                        best_match = block
                        best_match_score = match_score
                    found = True
            
            if found and best_match:
                # æ ‡è®°ä¸ºè§‚ç‚¹é«˜äº®ï¼ˆé»„è‰²ï¼‰
                highlight_item["type"] = "insight"
                results.append({
                    "block": best_match,
                    "analysis": {
                        "highlights": [highlight_item]
                    }
                })
            else:
                # è®°å½•æœªåŒ¹é…çš„æ–‡æœ¬
                unmatched.append(highlight_text[:80])
        
        # æ˜¾ç¤ºæœªåŒ¹é…çš„ç»Ÿè®¡
        if unmatched:
            print(f"   âš ï¸  æœ‰ {len(unmatched)} ä¸ªé«˜äº®æœªèƒ½ç²¾ç¡®å®šä½ï¼ˆå°†å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼‰")
            # åªæ˜¾ç¤ºå‰3ä¸ª
            for text in unmatched[:3]:
                print(f"      - {text}...")
        
        return results
    
    def _map_terms_to_blocks(self, analysis: Dict, text_blocks: List[Dict]) -> List[Dict]:
        """
        å°†æœ¯è¯­æ˜ å°„åˆ°å¯¹åº”çš„æ–‡æœ¬å—å’Œé¡µç ï¼ˆæ¯ä¸ªæœ¯è¯­åªæ ‡æ³¨ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œæ™ºèƒ½å»é‡ï¼‰
        
        Args:
            analysis: AIåˆ†æç»“æœ
            text_blocks: æ–‡æœ¬å—åˆ—è¡¨
            
        Returns:
            åŒ…å«ä½ç½®ä¿¡æ¯çš„ç»“æœåˆ—è¡¨
        """
        results = []
        unmatched = []
        seen_terms = set()  # ç”¨äºå»é‡ï¼Œè®°å½•å·²æ ‡æ³¨çš„æœ¯è¯­
        seen_term_list = []  # ä¿å­˜å®Œæ•´çš„æœ¯è¯­æ–‡æœ¬ï¼Œç”¨äºåŒ…å«å…³ç³»æ£€æŸ¥
        
        for term_item in analysis.get("terms", []):
            term_text = term_item.get("text", "")
            
            if not term_text:
                continue
            
            # å»é‡ï¼šå°†æœ¯è¯­è½¬ä¸ºå°å†™ä½œä¸ºå”¯ä¸€æ ‡è¯†
            term_key = term_text.lower().strip()
            
            # æ™ºèƒ½å»é‡ï¼šæ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æœ¯è¯­å†²çª
            should_skip = False
            term_to_remove = None
            
            # å¦‚æœå®Œå…¨ç›¸åŒï¼Œè·³è¿‡
            if term_key in seen_terms:
                should_skip = True
            else:
                # æ£€æŸ¥åŒ…å«å…³ç³»
                for seen_term in seen_term_list:
                    seen_key = seen_term.lower().strip()
                    
                    # å¦‚æœæ–°æœ¯è¯­æ˜¯å·²æœ‰æœ¯è¯­çš„å­ä¸²ï¼ˆå¦‚ "oxytocin" vs "oxytocin levels"ï¼‰
                    # ä¿ç•™æ›´é•¿çš„ç‰ˆæœ¬
                    if term_key in seen_key:
                        # æ–°æœ¯è¯­æ˜¯å­ä¸²ï¼Œè·³è¿‡æ–°æœ¯è¯­
                        should_skip = True
                        break
                    elif seen_key in term_key:
                        # å·²æœ‰æœ¯è¯­æ˜¯æ–°æœ¯è¯­çš„å­ä¸²ï¼Œç§»é™¤æ—§çš„ï¼Œæ·»åŠ æ–°çš„
                        term_to_remove = seen_term
                        break
            
            if should_skip:
                continue
            
            # å¦‚æœéœ€è¦ç§»é™¤æ—§æœ¯è¯­
            if term_to_remove:
                old_key = term_to_remove.lower().strip()
                if old_key in seen_terms:
                    seen_terms.remove(old_key)
                if term_to_remove in seen_term_list:
                    seen_term_list.remove(term_to_remove)
                # ä»resultsä¸­ç§»é™¤å¯¹åº”çš„ç»“æœ
                results = [r for r in results 
                          if r["analysis"]["highlights"][0]["text"].lower().strip() != old_key]
            
            # åœ¨æ–‡æœ¬å—ä¸­æŸ¥æ‰¾è¿™ä¸ªæœ¯è¯­ï¼ˆæ‰¾ç¬¬ä¸€æ¬¡å‡ºç°ï¼‰
            found = False
            best_match = None
            best_match_score = 0
            earliest_page = float('inf')
            
            for block in text_blocks:
                block_text = block["text"]
                page_num = block.get("page", 0)
                
                # æœ¯è¯­é€šå¸¸è¾ƒçŸ­ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…
                if self._smart_text_match(term_text, block_text):
                    # ä¼˜å…ˆé€‰æ‹©æ›´æ—©å‡ºç°çš„é¡µé¢
                    if page_num < earliest_page:
                        best_match = block
                        best_match_score = len(term_text)
                        earliest_page = page_num
                        found = True
                    elif page_num == earliest_page and len(term_text) > best_match_score:
                        # åŒä¸€é¡µé¢ï¼Œé€‰æ‹©æ›´é•¿çš„åŒ¹é…
                        best_match = block
                        best_match_score = len(term_text)
            
            if found and best_match:
                # æ ‡è®°ä¸ºå·²å¤„ç†
                seen_terms.add(term_key)
                seen_term_list.append(term_text)
                
                # æ„å»ºæœ¯è¯­çš„æ³¨é‡Šå†…å®¹ï¼ˆä¸­æ–‡ç¿»è¯‘ + è§£é‡Šï¼‰
                translation = term_item.get("translation", "")
                note = term_item.get("note", "")
                
                full_note = f"ã€æœ¯è¯­ã€‘{translation}"
                if note:
                    full_note += f"\n{note}"
                
                # æ ‡è®°ä¸ºæœ¯è¯­é«˜äº®ï¼ˆè“è‰²ï¼‰
                term_highlight = {
                    "text": term_text,
                    "note": full_note,
                    "type": "term"
                }
                
                results.append({
                    "block": best_match,
                    "analysis": {
                        "highlights": [term_highlight]
                    }
                })
            else:
                # è®°å½•æœªåŒ¹é…çš„æœ¯è¯­
                unmatched.append(term_text[:50])
        
        # æ˜¾ç¤ºç»Ÿè®¡
        if unmatched:
            print(f"   âš ï¸  æœ‰ {len(unmatched)} ä¸ªæœ¯è¯­æœªèƒ½å®šä½")
            # åªæ˜¾ç¤ºå‰3ä¸ª
            for text in unmatched[:3]:
                print(f"      - {text}")
        
        return results
    
    def analyze_document(self, text_blocks: List[Dict], progress_callback=None, custom_prompt: Optional[str] = None) -> List[Dict]:
        """
        åˆ†ææ•´ä¸ªæ–‡æ¡£ï¼ˆä½¿ç”¨å…¨æ–‡åˆ†æç­–ç•¥ï¼‰
        
        Args:
            text_blocks: æ–‡æœ¬å—åˆ—è¡¨ï¼ˆæ¥è‡ªPDFï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¯ä¸ªæ–‡æœ¬å—çš„åˆ†æç»“æœ
        """
        # ä½¿ç”¨æ–°çš„å…¨æ–‡åˆ†ææ–¹æ³•
        return self.analyze_document_full(text_blocks, progress_callback, custom_prompt)

